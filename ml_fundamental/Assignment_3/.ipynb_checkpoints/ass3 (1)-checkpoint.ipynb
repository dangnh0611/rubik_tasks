{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X3MLCYOR3Xc9"
   },
   "source": [
    "# Assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1j4xTfF01vL"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import random\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1tyqbTUw01vR"
   },
   "source": [
    "Function return the closet cluster centroid to each sample in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZD6bmhc401vT"
   },
   "outputs": [],
   "source": [
    "def findClosetCentroids(data,centroids):\n",
    "    \"\"\"Return the closet centroid index to each data point\"\"\"\n",
    "    nSample=data.shape[0]\n",
    "    #number of clusters\n",
    "    K=centroids.shape[0]\n",
    "    index=np.zeros((nSample,1))\n",
    "    temp=np.zeros((K,1))\n",
    "    for i in range(nSample):\n",
    "        for j in range(K):\n",
    "            temp[j]=np.sum((data[i,:]-centroids[j,:])**2)\n",
    "            index[i]=np.argmin(temp)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5xpcVMM801vW"
   },
   "source": [
    "Function to update the cluster's centroid base on the mean of members in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKV_9OEN01vY"
   },
   "outputs": [],
   "source": [
    "def updateCentroids(data,index,K):\n",
    "    \"\"\"Update the centroids\"\"\"\n",
    "    temp=np.zeros((K,data.shape[1]))\n",
    "    count=np.zeros((K,1))\n",
    "    for i in range(index.shape[0]):\n",
    "        temp[int(index[i])]+=data[i]\n",
    "        count[int(index[i])]+=1\n",
    "    for i in range(K):\n",
    "        if count[i,0]==0:\n",
    "            count[i,0]=1\n",
    "    centroids=temp/count\n",
    "    return centroids,count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xEBVsvVG01vb"
   },
   "source": [
    "Initialize centroids list randomlly, each one is picked in the data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UbijXQqD01vc"
   },
   "outputs": [],
   "source": [
    "def randomInitCentroids(data,K):\n",
    "    centroids=np.zeros((K,data.shape[1]))\n",
    "    for i in range(K):\n",
    "        flag=True\n",
    "        while flag:\n",
    "            flag=False\n",
    "            centroids[i]=data[np.random.randint(0,data.shape[0])]\n",
    "            for j in range(i):\n",
    "                if (centroids[i]==centroids[j]).all():\n",
    "                    flag=True\n",
    "                    break\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tEFRUxyO01vg"
   },
   "source": [
    "Implement K-mean clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aklRn7Dl01vh"
   },
   "outputs": [],
   "source": [
    "def KmeanClustering(data,K,epoch=500):\n",
    "    \"\"\"K-mean clustering implementation\"\"\"\n",
    "    centroids=randomInitCentroids(data,K)\n",
    "    for i in range(epoch):\n",
    "        index=findClosetCentroids(data,centroids)\n",
    "        centroids,count=updateCentroids(data,index,centroids.shape[0])\n",
    "    return centroids,count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nE8gc5J101vl"
   },
   "source": [
    "Logistic regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SQEccciS01vm"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def predict(self, x):\n",
    "        z = np.sum(x@self.W, axis=1)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    #fit Weight by a batch\n",
    "    def learn(self, x, y, learning_rate):\n",
    "        y_hat = self.predict(x).reshape((30,1))\n",
    "        dif=y-y_hat\n",
    "        new_W = x.T@dif\n",
    "        self.W = self.W + learning_rate * new_W\n",
    "        \n",
    "    def evaluate(self, x, y):\n",
    "        y_pred = self.predict(x).reshape(y.shape)\n",
    "        return {\n",
    "            \"loss\": -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)),\n",
    "            \"accuracy\": np.sum((y_pred > self.threshold).astype(int) == y) / y.shape[0]\n",
    "        }\n",
    "    \n",
    "    def fit(\n",
    "        self, x, y, x_valid = None, y_valid = None,\n",
    "        learning_rate = 0.001,\n",
    "        learning_rate_decay = 1,\n",
    "        batch_size = 32,\n",
    "        epoch = 1,\n",
    "        verbose = False\n",
    "    ):\n",
    "        self.W = np.random.rand(x.shape[1],1)\n",
    "        if x_valid is None:\n",
    "            x_valid = x\n",
    "        if y_valid is None:\n",
    "            y_valid = y\n",
    "        step = x.shape[0] // batch_size + (x.shape[0] % batch_size != 0)\n",
    "        metric_graph = {\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": []\n",
    "        }\n",
    "        for e in range(epoch):\n",
    "            for i in range(step):\n",
    "                self.learn(\n",
    "                    x[batch_size * i : batch_size * (i + 1),],\n",
    "                    y[batch_size * i : batch_size * (i + 1),],\n",
    "                    learning_rate\n",
    "                )\n",
    "                metrics = self.evaluate(x_valid, y_valid)\n",
    "                if (e <= 5 or (i + 1) == step) and verbose:\n",
    "                    metrics = self.evaluate(x_valid, y_valid)\n",
    "                    print(\"Epoch %d Step %d: Loss %f, Acc %f\" % (e + 1, i + 1, metrics[\"loss\"], metrics[\"accuracy\"]))\n",
    "            \n",
    "            metrics = self.evaluate(x_valid, y_valid)\n",
    "            metric_graph[\"loss\"].append(metrics[\"loss\"])\n",
    "            metric_graph[\"accuracy\"].append(metrics[\"accuracy\"])\n",
    "            learning_rate *= learning_rate_decay\n",
    "        \n",
    "        plt.plot(metric_graph[\"loss\"])\n",
    "        plt.title(\"Loss\")\n",
    "        plt.show()\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(metric_graph[\"accuracy\"])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d84Zw-KM01vp"
   },
   "source": [
    "Feature extraction using K-mean clustering.\n",
    "Finding the most dominant colors, sort by frequency, and then flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3006069,
     "status": "error",
     "timestamp": 1559485965857,
     "user": {
      "displayName": "Đăng Nguyễn Hồng",
      "photoUrl": "https://lh5.googleusercontent.com/-UTwxFgcfKfI/AAAAAAAAAAI/AAAAAAAAACY/UhcN1_QeVrs/s64/photo.jpg",
      "userId": "01453617082147037162"
     },
     "user_tz": -420
    },
    "id": "3xqXnyX801vq",
    "outputId": "f86c2e6b-738f-49fe-fd01-f3ac4fc05fb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 countryside/000125.jpeg\n",
      "[0.26255856 0.28090583 0.13367089 0.60093743 0.53007121 0.31498797\n",
      " 0.63941725 0.82881023 0.90813546]\n",
      "1 countryside/000211.jpeg\n",
      "[0.39322615 0.4600605  0.24861313 0.81142093 0.68455281 0.13590527\n",
      " 0.77492102 0.78719255 0.77850325]\n",
      "2 countryside/000247.jpg\n",
      "[0.84543825 0.83322914 0.83300495 0.64952306 0.55975151 0.44574735\n",
      " 0.2458893  0.24826968 0.20583823]\n",
      "3 countryside/000139.jpg\n",
      "[0.25739667 0.25815183 0.18163993 0.85423968 0.86018553 0.91275313\n",
      " 0.71673518 0.7257594  0.81446636]\n",
      "4 countryside/000428(1).jpg\n",
      "[0.67186764 0.76037736 0.8551116  0.41463855 0.40649514 0.23039751\n",
      " 0.62038143 0.61179193 0.58124627]\n",
      "5 countryside/000144.jpg\n"
     ]
    }
   ],
   "source": [
    "K=3\n",
    "nIterations=20\n",
    "\n",
    "country_filename=[os.path.join(\"countryside\",name) for name in os.listdir(\"countryside\")]\n",
    "metro_filename=[os.path.join(\"metropolitian\",name) for name in os.listdir(\"metropolitian\")]\n",
    "X=np.ones((len(country_filename)+len(metro_filename),3*K+1))\n",
    "y=np.zeros((len(country_filename)+len(metro_filename),1))\n",
    "\n",
    "for idx,filename in enumerate(country_filename+metro_filename):\n",
    "    print(idx,filename)\n",
    "    img=skimage.io.imread(filename)\n",
    "    img=skimage.transform.resize(img,(img.shape[0]//16,img.shape[1]//16,3))\n",
    "    data=img.reshape(-1,3)\n",
    "    centroids,count=KmeanClustering(data,K,nIterations)\n",
    "    #sorted by color frequency\n",
    "    sorted_pair=sorted(zip(count,centroids),reverse=True)\n",
    "    sorted_centroids=[pair[1] for pair in sorted_pair]\n",
    "    centroids=np.concatenate(sorted_centroids)\n",
    "    print(centroids)\n",
    "    X[idx,1:]=centroids\n",
    "    #label sample\n",
    "    if filename[0]=='c':\n",
    "        y[idx]=1\n",
    "    else:\n",
    "        y[idx]=0\n",
    "#training and validation set splitting: 75% for training and 25% for validation set\n",
    "x,x_valid,y,y_valid=model_selection.train_test_split(X,y,test_size=0.25)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2dd55v_qHFWP"
   },
   "outputs": [],
   "source": [
    "print(centroids.shape)\n",
    "print(centroids_dict)\n",
    "print(count)\n",
    "print(sorted_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hXzZrYNv1Ay_"
   },
   "source": [
    "Logistic training and reporting accuracy on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Q9XTz6V01vy"
   },
   "outputs": [],
   "source": [
    "model=LogisticRegression(0.5)\n",
    "model.fit(x,y,x_valid,y_valid,epoch=500,verbose=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ass3.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
